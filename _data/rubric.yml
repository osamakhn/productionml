  - title: Tests for Features &amp; Data
    questions:
      - id: 1-1
        text: Test that the distributions of each feature match your expectations
        hint: One example might be to test that Feature A takes on values 1 to 5, or that the two most common values of Feature B are 'Harry' and 'Potter' and they account for 10% of all values. This test can fail due to real external changes, which may require changes in your model.

      - id: 1-2
        text: Test the relationship between each feature and the target, and the pairwise correlations be- tween individual signals
        hint: It is important to have a thorough understanding of the individual features used in a given model; this is a minimal set of tests, more exploration may be needed to develop a full understanding. These tests may be run by computing correlation coefficients, by training models with one or two features, or by training a set of models that each have one of k features individually removed.
      
      - id: 1-3
        text: Test the cost of each feature
        hint: The costs of a feature may include added inference latency and RAM usage, more upstream data dependencies, and additional expected instability incurred by relying on that feature. Consider whether this cost is worth paying when traded off against the provided improvement in model quality.

      - id: 1-4
        text: Test that a model does not contain any features that have been manually determined as unsuit- able for use
        hint: A feature might be unsuitable when it’s been discovered to be unreliable, overly expensive, etc. Tests are needed to ensure that such features are not accidentally included (e.g. via copy-paste) into new models.

      - id: 1-5
        text: Test that your system maintains privacy controls across its entire data pipeline
        hint: While strict access control is typically maintained on raw data, ML systems often export and transform that data during training. Test to ensure that access control is appropriately restricted across the entire pipeline.

      - id: 1-6
        text: Test the calendar time needed to develop and add a new feature to the production model
        hint: The faster a team can go from a feature idea to it running in production, the faster it can both improve the system and respond to external changes.

      - id: 1-7
        text: Test all code that creates input features, both in training and serving
        hint: It can be tempting to believe feature creation code is simple enough to not need unit tests, but this code is crucial for correct behavior and so its continued quality is vital.

  - title: Tests for Model Development
    questions:
      - id: 2-1
        text: Test that every model specification undergoes a code review and is checked in to a repository
        hint: It can be tempting to avoid, but disciplined code review remains an excellent method for avoiding silly errors and for enabling more efficient incident response and debugging.
      
      - id: 2-2
        text: Test the relationship between offline proxy metrics and the actual impact metrics
        hint: For exam- ple, how does a one-percent improvement in accuracy or AUC translate into effects on metrics of user satisfaction, such as click through rates? This can be measured in a small scale A/B experiment using an intentionally degraded model.

      - id: 2-3
        text: Test the impact of each tunable hyperparameter
        hint: Methods such as a grid search or a more sophisticated hyperparameter search strategy not only improve predictive performance, but also can uncover hidden reliability issues. For example, it can be surprising to observe the impact of massive increases in data parallelism on model accuracy.

      - id: 2-4
        text: Test the effect of model staleness
        hint: If predictions are based on a model trained yesterday versus last week versus last year, what is the impact on the live metrics of interest? All models need to be updated eventually to account for changes in the external world; a careful assessment is important to guide such decisions.
      
      - id: 2-5
        text: Test against a simpler model as a baseline
        hint: Regularly testing against a very simple baseline model, such as a linear model with very few features, is an effective strategy both for confirming the functionality of the larger pipeline and for helping to assess the cost to benefit tradeoffs of more sophisticated techniques.

      - id: 2-6
        text: Test model quality on important data slices
        hint: Slicing a data set along certain dimensions of interest provides fine-grained understanding of model performance. For example, important slices might be users by country or movies by genre. Examining sliced data avoids having fine-grained performance issues masked by a global summary metric.

      - id: 2-7
        text: Test the model for implicit bias
        hint: This may be viewed as an extension of examining important data slices, and may reveal issues that can be root-caused and addressed. For example, implicit bias might be induced by a lack of sufficient diversity in the training data.

  - title: Tests for ML Infrastructure
    questions:
      - id: 3-1
        text: Test the reproducibility of training
        hint: Train two models on the same data, and observe any differ- ences in aggregate metrics, sliced metrics, or example-by-example predictions. Large differences due to non-determinism can exacerbate debugging and troubleshooting.
      
      - id: 3-2
        text: Unit test model specification code
        hint: Although model specifications may seem like “configuration”, such files can have bugs and need to be tested. Useful assertions include testing that training results in decreased loss and that a model can restore from a checkpoint after a mid-training job crash.

      - id: 3-3
        text: Integration test the full ML pipeline
        hint: A good integration test runs all the way from original data sources, through feature creation, to training, and to serving. An integration test should run both continuously as well as with new releases of models or servers, in order to catch problems well before they reach production.

      - id: 3-4
        text: Test model quality before attempting to serve it
        hint: Useful tests include testing against data with known correct outputs and validating the aggregate quality, as well as comparing predictions to a previous version of the model.

      - id: 3-5
        text: Test that a single example or training batch can be sent to the model, and changes to internal state can be observed from training through to prediction
        hint: Observing internal state on small amounts of data is a useful debugging strategy for issues like numerical instability.
      
      - id: 3-6
        text: Test models via a canary process before they enter production serving environments
        hint: Model- ing code can change more frequently than serving code, so there is a danger that an older serving system will not be able to serve a model trained from newer code. This includes testing that a model can be loaded into the production serving binaries and perform inference on production input data at all. It also includes a canary process, in which a new version is tested on a small trickle of live data.

      - id: 3-7
        text: Test how quickly and safely a model can be rolled back to a previous serving version
        hint: A model “roll back” procedure is useful in cases where upstream issues might result in unexpected changes to model quality. Being able to quickly revert to a previous known-good state is as crucial with ML models as with any other aspect of a serving system.

  - title: Monitoring Tests for ML
    questions:
      - id: 4-1
        text: Test for upstream instability in features, both in training and serving
        hint: Upstream instability can create problems both at training and serving (inference) time. Training time instability is especially problematic when models are updated or retrained frequently. Serving time instability can occur even when the models themselves remain static. As examples, what alert would fire if one datacenter stops sending data? What if an upstream signal provider did a major version upgrade?

      - id: 4-2
        text: Test that data invariants hold in training and serving inputs
        hint: For example, test if Feature A and Feature B should always have the same number of non-zero values in each example, or that Feature C is always in the range (0, 100) or that class distribution is about 10:1.

      - id: 4-3
        text: Test that your training and serving features compute the same values
        hint: The codepaths that actually generate input features may differ for training and inference time, due to tradeoffs for flexibility vs. efficiency and other concerns. This is sometimes called “training/serving skew” and requires careful monitoring to detect and avoid.
      
      - id: 4-4
        text: Test for model staleness
        hint: For models that continually update, this means monitoring staleness throughout the training pipeline, to be able to determine in the case of a stale model where the pipeline has stalled. For example, if a daily job stopped generating an important table, what alert would fire?

      - id: 4-5
        text: Test for NaNs or infinities appearing in your model during training or serving
        hint: Invalid numeric values can easily crop up in your learning model, and knowing that they have occurred can speed diagnosis of the problem.

      - id: 4-6
        text: Test for dramatic or slow-leak regressions in training speed, serving latency, throughput, or RAM usage
        hint: The computational performance (as opposed to predictive quality) of an ML system is often a key concern at scale, and should be monitored via specialized regression testing. Dramatic regressions and slow regressions over time may require different kinds of monitoring.

      - id: 4-7
        text: Test for regressions in prediction quality on served data
        hint: For many systems, monitoring for nonzero bias can be an effective canary for identifying real problems, though it may also result from changes in the world.